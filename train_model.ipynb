{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omerdoruksuder/anaconda3/envs/master-thesis/lib/python3.11/site-packages/exchange_calendars/exchange_calendar.py:2347: FutureWarning: 'T' is deprecated and will be removed in a future version. Please use 'min' instead of 'T'.\n",
      "  align: pd.Timedelta | str = pd.Timedelta(1, \"T\"),\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "from train import *\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment settings\n",
    "Settings are based on the training procedure that produced the final generator used in evaluating the stylized facts and training the RL agent in portfolio management.  \n",
    "However, using different machines would result in cumulative differences in computation results likely due to varied machine precision.  \n",
    "Original experiments were performed on machine with following specs:\n",
    "1. OS: Ubuntu 20.04.1\n",
    "2. CPU: Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz\n",
    "3. GPU: Tesla V100-SXM2-32GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "batch_size = 38 # number of samples in each batch\n",
    "sample_len = 300 # length of each sample\n",
    "sample_model = 'Realdt' # GBM, Heston, OU, RealData, Realdt, spx_rates\n",
    "lead_lag = True # whether to use lead lag transformation\n",
    "lags = [1] # number of lags to use for lead lag transformation: int or list[int]\n",
    "seed = 42\n",
    "\n",
    "# real data parameters\n",
    "stride = 50 # for real data\n",
    "start_date = '1995-01-01' # start date for real data\n",
    "end_date = '2018-09-18' # end date for real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and kernel related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature kernel\n",
    "static_kernel_type = 'rq' # type of static kernel to use - rbf, rbfmix, rq, rqmix, rqlinear for\n",
    "n_levels = 10 # number of levels in the truncated signature kernel\n",
    "\n",
    "# generator\n",
    "seq_dim = 1 # dimension of sequence vector\n",
    "activation = 'Tanh' # pytorch names e.g. Tanh, ReLU. NOTE: does NOT change transformer layers'\n",
    "hidden_size = 64\n",
    "n_lstm_layers = 1 # number of LSTM layers\n",
    "conditional = True # feed in history for LSTM generators\n",
    "hist_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 4 # dimension of noise vector\n",
    "ma = True # whether to use MA noise generator fitted to log returns gaussianized by Lambert W transformation\n",
    "ma_p = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000 # number of batches\n",
    "start_lr = 0.001 # starting learning rate\n",
    "patience = 100 # number of epochs to wait before reducing lr\n",
    "lr_factor = 0.5 # factor to multiply lr by for scheduler\n",
    "early_stopping = patience*3 # number of epochs to wait before no improvement\n",
    "kernel_sigma = 0.1 # starting kernel_sigma\n",
    "num_losses = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to tensorboard log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all parameters to a dictionary\n",
    "rng = np.random.default_rng(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "data_params, model_params, train_params = get_params_dicts(vars().copy())\n",
    "\n",
    "# save parameters to tensorboard\n",
    "writer = start_writer(data_params, model_params, train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data, kernel, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 5855.9883370939215\n",
      "            Iterations: 36\n",
      "            Function evaluations: 837\n",
      "            Gradient evaluations: 36\n",
      "                        Zero Mean - ARCH Model Results                        \n",
      "==============================================================================\n",
      "Dep. Variable:           gaussianized   R-squared:                       0.000\n",
      "Mean Model:                 Zero Mean   Adj. R-squared:                  0.000\n",
      "Vol Model:                       ARCH   Log-Likelihood:               -5855.99\n",
      "Distribution:                  Normal   AIC:                           11754.0\n",
      "Method:            Maximum Likelihood   BIC:                           11897.9\n",
      "                                        No. Observations:                 6999\n",
      "Date:                Wed, Feb 11 2026   Df Residuals:                     6999\n",
      "Time:                        14:37:30   Df Model:                            0\n",
      "                               Volatility Model                              \n",
      "=============================================================================\n",
      "                 coef    std err          t      P>|t|       95.0% Conf. Int.\n",
      "-----------------------------------------------------------------------------\n",
      "omega          0.0727  8.049e-03      9.032  1.687e-19  [5.692e-02,8.848e-02]\n",
      "alpha[1]       0.0186  1.285e-02      1.449      0.147 [-6.565e-03,4.380e-02]\n",
      "alpha[2]       0.0827  1.292e-02      6.401  1.547e-10    [5.736e-02,  0.108]\n",
      "alpha[3]       0.0258  1.273e-02      2.028  4.253e-02  [8.698e-04,5.075e-02]\n",
      "alpha[4]       0.0722  1.427e-02      5.061  4.160e-07    [4.427e-02,  0.100]\n",
      "alpha[5]       0.1752  1.634e-02     10.721  8.124e-27      [  0.143,  0.207]\n",
      "alpha[6]       0.0267  1.268e-02      2.101  3.563e-02  [1.791e-03,5.151e-02]\n",
      "alpha[7]       0.0282  1.428e-02      1.978  4.793e-02  [2.572e-04,5.622e-02]\n",
      "alpha[8]       0.0191  1.346e-02      1.419      0.156 [-7.275e-03,4.547e-02]\n",
      "alpha[9]       0.0556  1.477e-02      3.764  1.672e-04  [2.664e-02,8.452e-02]\n",
      "alpha[10]      0.1145  1.513e-02      7.571  3.715e-14    [8.487e-02,  0.144]\n",
      "alpha[11]      0.0196  1.386e-02      1.414      0.157 [-7.572e-03,4.676e-02]\n",
      "alpha[12]  4.4773e-03  1.334e-02      0.336      0.737 [-2.166e-02,3.061e-02]\n",
      "alpha[13]  2.2236e-11  1.147e-02  1.938e-09      1.000 [-2.248e-02,2.248e-02]\n",
      "alpha[14]      0.0266  1.424e-02      1.868  6.182e-02 [-1.315e-03,5.449e-02]\n",
      "alpha[15]      0.0318  1.366e-02      2.329  1.985e-02  [5.042e-03,5.858e-02]\n",
      "alpha[16]  2.2374e-11  1.425e-02  1.570e-09      1.000 [-2.793e-02,2.793e-02]\n",
      "alpha[17]  2.6772e-11  1.227e-02  2.182e-09      1.000 [-2.405e-02,2.405e-02]\n",
      "alpha[18]  3.6858e-11  1.414e-02  2.607e-09      1.000 [-2.771e-02,2.771e-02]\n",
      "alpha[19]      0.0806  1.513e-02      5.331  9.783e-08    [5.099e-02,  0.110]\n",
      "alpha[20]      0.0205  1.290e-02      1.588      0.112 [-4.794e-03,4.576e-02]\n",
      "=============================================================================\n",
      "\n",
      "Covariance estimator: robust\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenLSTM(\n",
       "  (rnn): LSTM(6, 64, batch_first=True)\n",
       "  (mean_net): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (var_net): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (output_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataloader = get_dataloader(**{**data_params, **model_params})\n",
    "kernel = get_signature_kernel(**{**model_params, **train_params})\n",
    "generator = get_generator(**{**model_params, **data_params})\n",
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MMD-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omerdoruksuder/anaconda3/envs/master-thesis/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      " 33%|███▎      | 1/3 [00:29<00:59, 29.57s/it]"
     ]
    }
   ],
   "source": [
    "train(generator, kernel, dataloader, rng, writer, device, **{**train_params, **model_params, **data_params})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
