{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4b56d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sigkernel as ksig_pde\n",
    "import sigkernel_ as ksig_disc\n",
    "import utils.data\n",
    "from generators.generators import *\n",
    "from generators.ESN import ESNGenerator\n",
    "from models.ESN_MMD_trainer import train_ESN_MMD\n",
    "from sigkernel_.loss import compute_mmd_loss\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dtype=torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a37b6",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b930450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discretized signature kernel\n",
    "static_kernel_type    = 'rbf' # type of static kernel to use - rbf, rbfmix, rq, rqmix, rqlinear for\n",
    "n_levels              = 10 # number of levels in the truncated signature kernel\n",
    "disc_sig_kernel_sigma = 1e-5 # bandwidth parameter for the static kernel\n",
    "\n",
    "kwargs_disc_sig = {\n",
    "    'static_kernel_type': static_kernel_type,\n",
    "    'n_levels': n_levels,\n",
    "    'kernel_sigma': disc_sig_kernel_sigma,\n",
    "}\n",
    "\n",
    "sig_disc_kernel = ksig_disc.kernels.get_discretized_signature_kernel(**kwargs_disc_sig)\n",
    "\n",
    "#---------------------------------\n",
    "# Define the pde signature kernel\n",
    "pde_sig_sigma = 1e1 # bandwidth parameter for the static kernel\n",
    "static_kernel = ksig_pde.RBFKernel(sigma=pde_sig_sigma) # define static kernel\n",
    "\n",
    "# Initialize the corresponding signature kernel\n",
    "sig_pde_kernel = ksig_pde.SigKernel(static_kernel, dyadic_order=0)\n",
    "\n",
    "#---------------------------------\n",
    "# Define static kernel\n",
    "static_sigma = 1e3 # bandwidth parameter for the static kernel\n",
    "rbf_kernel = ksig_disc.kernels.RBFKernel(sigma=static_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad9221fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using kernel: SigKernel with kernel mode: sequential\n"
     ]
    }
   ],
   "source": [
    "# choose one:\n",
    "# kernel = sig_disc_kernel      # discretized signature kernel\n",
    "kernel = sig_pde_kernel        # pde signature kernel\n",
    "# kernel = rbf_kernel           # static kernel\n",
    "\n",
    "if \"sig\" in kernel.__class__.__name__.lower() or \"volt\" in kernel.__class__.__name__.lower():\n",
    "    kernel_mode = \"sequential\"\n",
    "else:\n",
    "    kernel_mode = \"static\"\n",
    "\n",
    "print(f\"Using kernel: {kernel.__class__.__name__} with kernel mode: {kernel_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd36fab",
   "metadata": {},
   "source": [
    "# Generator and ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4d3219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and ESN details ----------------------\n",
    "T = 200 # length of time series\n",
    "N = 20  # number of samples\n",
    "d = 1   # dimension of time series\n",
    "p = 2   # AR order\n",
    "q = 0   # MA order\n",
    "phi = [0.7, -0.2] # AR coefficients\n",
    "theta = None # MA coefficients\n",
    "\n",
    "h, m, d = 500, 20, 1 # ESN hyperparameters: reservoir size, input dimension, output dimension\n",
    "A = 0.9 * torch.randn(h, h) / (h ** 0.5) # ESN reservoir weight matrix\n",
    "C = torch.randn(h, m) / (m ** 0.5) # ESN input weight matrix\n",
    "\n",
    "# Define data generator and esn\n",
    "target_generator = ARMA(T=T, p=p, q=q, phi=phi, theta=theta)\n",
    "esn = ESNGenerator(A, C, out_dim=d, xi_scale=1.0, eta_scale=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e053b",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "219b9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\": N,\n",
    "    \"T\": T,\n",
    "    \"d\": d,\n",
    "    \"esn\": esn,\n",
    "    \"target_generator\": target_generator,\n",
    "    \"kernel\": kernel,\n",
    "    \"kernel_mode\": kernel_mode,\n",
    "}\n",
    "\n",
    "optim_params = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_factor\": 0.5,\n",
    "    \"plateau_patience\": 100,\n",
    "    \"max_lr_drops\": 3,\n",
    "    \"early_stopping_patience\": 200,\n",
    "    \"min_lr\": 1e-6,\n",
    "}\n",
    "\n",
    "loss_params = {\n",
    "    \"lead_lag\": False,\n",
    "    \"lags\": 1,\n",
    "    \"num_losses\": 20,\n",
    "}\n",
    "\n",
    "io_params = {\n",
    "    \"out_dir\": \"./runs\",\n",
    "    \"run_name\": \"esn_vs_ar2\",\n",
    "    \"save_every\": 50,\n",
    "}\n",
    "\n",
    "misc_params = {\n",
    "    \"dtype\": torch.float64,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "    **train_params,\n",
    "    **optim_params,\n",
    "    **loss_params,\n",
    "    **io_params,\n",
    "    **misc_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3576e65",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20e2de14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   3%|▎         | 29/1000 [06:40<6:01:02, 22.31s/it, avg_k=13, drops=0, loss=3.47, lr=0.001]  [E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "train:   3%|▎         | 29/1000 [07:01<3:55:09, 14.53s/it, avg_k=13, drops=0, loss=3.47, lr=0.001][E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:130] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ESN_MMD\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/work shared/NTU/RESEARCH/_github_repos/My_trial_space/models/ESN_MMD_trainer.py:195\u001b[0m, in \u001b[0;36mtrain_ESN_MMD\u001b[0;34m(esn, kernel, kernel_mode, T, epochs, batch_size, dataloader, target_generator, device, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     Xk, Zk \u001b[38;5;241m=\u001b[39m X, Z\n\u001b[0;32m--> 195\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_mmd_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead_lag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    198\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/work shared/NTU/RESEARCH/_github_repos/My_trial_space/sigkernel_/loss.py:28\u001b[0m, in \u001b[0;36mcompute_mmd_loss\u001b[0;34m(kernel, X, output, lead_lag, lags)\u001b[0m\n\u001b[1;32m     26\u001b[0m     X \u001b[38;5;241m=\u001b[39m batch_lead_lag_transform(X[:,:,\u001b[38;5;241m1\u001b[39m:], X[:,:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m], lags) \u001b[38;5;66;03m# inputs are (price series, time dimension, lags to use)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     output \u001b[38;5;241m=\u001b[39m batch_lead_lag_transform(output[:,:,\u001b[38;5;241m1\u001b[39m:], output[:,:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m], lags)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmmd_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/work shared/NTU/RESEARCH/_github_repos/My_trial_space/sigkernel_/loss.py:13\u001b[0m, in \u001b[0;36mmmd_loss\u001b[0;34m(X, Y, kernel)\u001b[0m\n\u001b[1;32m     11\u001b[0m Kxx \u001b[38;5;241m=\u001b[39m gram(kernel, X, X)\n\u001b[1;32m     12\u001b[0m Kyy \u001b[38;5;241m=\u001b[39m gram(kernel, Y, Y)\n\u001b[0;32m---> 13\u001b[0m Kxy \u001b[38;5;241m=\u001b[39m \u001b[43mgram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m n \u001b[38;5;241m=\u001b[39m Kxx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m m \u001b[38;5;241m=\u001b[39m Kyy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/work shared/NTU/RESEARCH/_github_repos/My_trial_space/sigkernel_/kernels.py:153\u001b[0m, in \u001b[0;36mgram\u001b[0;34m(kernel, X, Y)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03mReturns Gram matrix K(X,Y) for either:\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m  - callable kernel: kernel(X,Y)\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m  - object with compute_Gram: kernel.compute_Gram(X,Y)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(kernel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_Gram\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_Gram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(kernel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_gram\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel\u001b[38;5;241m.\u001b[39mcompute_gram(X, Y)\n",
      "File \u001b[0;32m~/anaconda3/envs/master-thesis/lib/python3.11/site-packages/sigkernel/sigkernel.py:102\u001b[0m, in \u001b[0;36mSigKernel.compute_Gram\u001b[0;34m(self, X, Y, sym, max_batch)\u001b[0m\n\u001b[1;32m    100\u001b[0m batch_Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_X \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_batch \u001b[38;5;129;01mand\u001b[39;00m batch_Y \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_batch:\n\u001b[0;32m--> 102\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[43m_SigKernelGram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdyadic_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msym\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_naive_solver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m batch_X \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_batch \u001b[38;5;129;01mand\u001b[39;00m batch_Y \u001b[38;5;241m>\u001b[39m max_batch:\n\u001b[1;32m    104\u001b[0m     cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch_Y\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/master-thesis/lib/python3.11/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/master-thesis/lib/python3.11/site-packages/sigkernel/sigkernel.py:352\u001b[0m, in \u001b[0;36m_SigKernelGram.forward\u001b[0;34m(ctx, X, Y, static_kernel, dyadic_order, sym, _naive_solver)\u001b[0m\n\u001b[1;32m    349\u001b[0m NN \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdyadic_order)\u001b[38;5;241m*\u001b[39m(N\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# computing dsdt k(X^i_s,Y^j_t)\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m G_static \u001b[38;5;241m=\u001b[39m \u001b[43mstatic_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGram_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m G_static_ \u001b[38;5;241m=\u001b[39m G_static[:, :, \u001b[38;5;241m1\u001b[39m:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m G_static[:, :, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m G_static[:, :, \u001b[38;5;241m1\u001b[39m:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m G_static[:, :, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    354\u001b[0m G_static_ \u001b[38;5;241m=\u001b[39m tile(tile(G_static_, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdyadic_order)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdyadic_order), \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdyadic_order)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdyadic_order)\n",
      "File \u001b[0;32m~/anaconda3/envs/master-thesis/lib/python3.11/site-packages/sigkernel/static_kernels.py:73\u001b[0m, in \u001b[0;36mRBFKernel.Gram_matrix\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     71\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.\u001b[39m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mipk,jqk->ijpq\u001b[39m\u001b[38;5;124m'\u001b[39m, X, Y)\n\u001b[1;32m     72\u001b[0m dist \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(Xs,(A,\u001b[38;5;241m1\u001b[39m,M,\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(Ys,(\u001b[38;5;241m1\u001b[39m,B,\u001b[38;5;241m1\u001b[39m,N))\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = train_ESN_MMD(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
